{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_funsd_parent_child_based.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "YQFgr5EutIGe",
        "UJYyF8y5tN9h",
        "fal52emMue4w"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhUKsCXNxepB"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "  global drive\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "  authenticate()\n",
        "  \n",
        "  for fileId in fileIds:    \n",
        "    \n",
        "    downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "    downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7NnNK_cUh_p"
      },
      "source": [
        "## Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3DG91uxXXus"
      },
      "source": [
        "def relation_naming(label1,label2):\n",
        "  if label1=='question' and label2=='answer':\n",
        "    return 'parent'\n",
        "  elif label1=='answer' and label2=='question':\n",
        "    return 'child'\n",
        "  elif label1=='header' and label2=='question':\n",
        "    return 'parent'\n",
        "  elif label1=='question' and label2=='header':\n",
        "    return 'child'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQFgr5EutIGe"
      },
      "source": [
        "## Import the scene graph file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzwNedliyHp5"
      },
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"training_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"training_dataset.pkl\", \"1jdLid7PcW8Wa8EjvnWZzaRFmaYEoGoE0\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"testing_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"testing_dataset.pkl\", \"1-1FTOU_0ax5iQ0VapQNQU4aJdE5_pmvW\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeCQ9knU7P9z"
      },
      "source": [
        "import pickle\n",
        "with open('training_dataset.pkl', 'rb') as f:\n",
        "    train_list_dict=pickle.load(f)\n",
        "with open('testing_dataset.pkl', 'rb') as f:\n",
        "    eval_list_dict=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-zlgoHxLQD6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0tYG8g81u9Z"
      },
      "source": [
        "##Import Some External Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJYyF8y5tN9h"
      },
      "source": [
        "## Define some pre-processing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7QYVgYFuQtI"
      },
      "source": [
        "print(train_list_dict['0000971160']['objects']['1'].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P55FJGdtDCc"
      },
      "source": [
        "# according to id to search text\n",
        "bbox_id_text = {}\n",
        "for l in train_list_dict:\n",
        "  for obj in train_list_dict[l]['objects']:\n",
        "    id = train_list_dict[l]['objects'][obj]['id']\n",
        "    bbox_id_text[id] = {}\n",
        "    bbox_id_text[id]['text'] = train_list_dict[l]['objects'][obj]['text']\n",
        "for l in eval_list_dict:\n",
        "  for obj in eval_list_dict[l]['objects']:\n",
        "    id = eval_list_dict[l]['objects'][obj]['id']\n",
        "    bbox_id_text[id] = {}\n",
        "    bbox_id_text[id]['text'] = eval_list_dict[l]['objects'][obj]['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwmFBI2tF5L"
      },
      "source": [
        "# according to id to search object id in a document\n",
        "def globalid_to_localid(id):\n",
        "  for l in train_list_dict:\n",
        "    for obj in train_list_dict[l]['objects']:\n",
        "      if id == train_list_dict[l]['objects'][obj]['id']:\n",
        "        return obj\n",
        "  for l in eval_list_dict:\n",
        "    for obj in eval_list_dict[l]['objects']:\n",
        "      if id == eval_list_dict[l]['objects'][obj]['id']:\n",
        "        return obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYDBL-DYuMdy"
      },
      "source": [
        "# this dictionary used to transfer object id to text_density/text_number/char_density/char_number/visual_embedding/\n",
        "id_density_dict = {}\n",
        "for l in eval_list_dict:\n",
        "  id_density_dict[l] = {}\n",
        "  for obj in eval_list_dict[l]['objects']:\n",
        "    id_density_dict[l][obj] = eval_list_dict[l]['objects'][obj]['bert_large_emb']\n",
        "for l in train_list_dict:\n",
        "  id_density_dict[l] = {}\n",
        "  for obj in train_list_dict[l]['objects']:\n",
        "    id_density_dict[l][obj] = train_list_dict[l]['objects'][obj]['bert_large_emb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVuME0kfzbr8"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "# one dimensional feature embedding\n",
        "def positionalencoding1d(d_model, feature_list):\n",
        "    \"\"\"\n",
        "    :param d_model: dimension of the model\n",
        "    :param feature_list: length of positions\n",
        "    :return: length*d_model position matrix\n",
        "    \"\"\"\n",
        "    if d_model % 2 != 0:\n",
        "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                         \"odd dim (got dim={:d})\".format(d_model))\n",
        "    pe = torch.zeros(len(feature_list), d_model)\n",
        "    feats = torch.tensor(feature_list).unsqueeze(1)\n",
        "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
        "                         -(math.log(10000.0) / d_model)))\n",
        "    pe[:, 0::2] = torch.sin(feats.float() * div_term)\n",
        "    pe[:, 1::2] = torch.cos(feats.float() * div_term)\n",
        "    pe = pe.tolist()\n",
        "    return pe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ot2qaDtTCI"
      },
      "source": [
        "## Extracting Information from training and evaluation json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4DlyOn7tcd"
      },
      "source": [
        "train_list_dict['01073843']['objects']['0'].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTanPUdYskxb"
      },
      "source": [
        "# Generate local graph based training dataset objects and relations list\n",
        "new_train_list_dict = {}\n",
        "for l in train_list_dict:\n",
        "  tem_dic = {}\n",
        "  tem_dic['objects'] = []\n",
        "  tem_dic['relationships'] = []\n",
        "  \n",
        "  for obj in train_list_dict[l]['objects']:\n",
        "    tem_dic['objects'].append(obj)\n",
        "    for rel in train_list_dict[l]['objects'][obj]['relations']:\n",
        "      tem_rel = [obj]\n",
        "      obj2_id = train_list_dict[l]['objects'][obj]['relations'][rel]['object']\n",
        "      obj2_id= globalid_to_localid(obj2_id)\n",
        "      tem_rel.append(obj2_id)\n",
        "      label1 = train_list_dict[l]['objects'][obj]['category']\n",
        "      label2 = train_list_dict[l]['objects'][obj2_id]['category']\n",
        "      rel_name = relation_naming(label1,label2)\n",
        "      tem_rel.append(rel_name)\n",
        "      tem_dic['relationships'].append(tem_rel)\n",
        "  new_train_list_dict[l] = tem_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUqo9tC3_B9R"
      },
      "source": [
        "# Generate local graph based validation dataset objects and relations list\n",
        "new_eval_list_dict = {}\n",
        "for l in eval_list_dict:\n",
        "  tem_dic = {}\n",
        "  tem_dic['objects'] = []\n",
        "  tem_dic['relationships'] = []\n",
        "\n",
        "  for obj in eval_list_dict[l]['objects']:\n",
        "    tem_dic['objects'].append(obj)\n",
        "    for rel in eval_list_dict[l]['objects'][obj]['relations']:\n",
        "      tem_rel = [obj]\n",
        "      obj2_id = eval_list_dict[l]['objects'][obj]['relations'][rel]['object']\n",
        "      obj2_id= globalid_to_localid(obj2_id)\n",
        "      tem_rel.append(obj2_id)\n",
        "      label1 = eval_list_dict[l]['objects'][obj]['category']\n",
        "      label2 = eval_list_dict[l]['objects'][obj2_id]['category']\n",
        "      rel_name = relation_naming(label1,label2)\n",
        "      tem_rel.append(rel_name)\n",
        "      tem_dic['relationships'].append(tem_rel)\n",
        "  new_eval_list_dict[l] = tem_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU48YYtyxgkb"
      },
      "source": [
        "print(train_list_dict['92039708_9710']['objects']['4'])\n",
        "print(train_list_dict['92039708_9710']['objects']['1']['bert_large_emb'])\n",
        "print(new_train_list_dict['92039708_9710']['relationships'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FwWclhSu0Ip"
      },
      "source": [
        "#Using for searching real node size\n",
        "num_obj_dict = {}\n",
        "for l in new_train_list_dict:\n",
        "    num_obj_dict[l] = len(new_train_list_dict[l]['objects'])\n",
        "for l in new_eval_list_dict:\n",
        "    num_obj_dict[l] = len(new_eval_list_dict[l]['objects'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUc-r08pvQhu"
      },
      "source": [
        "print(num_obj_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7TKyvIo0WgS"
      },
      "source": [
        "# only rename\n",
        "list_dict_train={}\n",
        "for l in new_train_list_dict:\n",
        "    list_dict_train[l] = new_train_list_dict[l]\n",
        "list_dict_test = {}\n",
        "for l in new_eval_list_dict:\n",
        "    list_dict_test[l] = new_eval_list_dict[l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgahG2e8pAGx"
      },
      "source": [
        "## Training and validation dataframe generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyZaDNCsAH_2"
      },
      "source": [
        "#generate the training and labeling information for each image\n",
        "#this information can be used to transfer object id to corresponding visual, text density or other kinds of features.\n",
        "density_list_train = []\n",
        "label_list_train = []\n",
        "img_list_train = []\n",
        "label_dict_train = {}\n",
        "for l in train_list_dict:\n",
        "  label_dict_train[l] = []\n",
        "  for obj in train_list_dict[l]['objects']:\n",
        "    density_list_train.append(obj)\n",
        "    label_list_train.append(train_list_dict[l]['objects'][obj]['category'])\n",
        "    img_list_train.append(l)\n",
        "    label_dict_train[l].append(train_list_dict[l]['objects'][obj]['category'])\n",
        "  #padding\n",
        "  for obj in range(num_obj_dict[l],181):\n",
        "    density_list_train.append(str(obj))\n",
        "    label_list_train.append(str(-1))\n",
        "    img_list_train.append(l)\n",
        "    label_dict_train[l].append(str(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3JfDgcmBQ0V"
      },
      "source": [
        "# generating evalutaion labeling informaiton dictionary\n",
        "density_list_eval = []\n",
        "label_list_eval = []\n",
        "label_dict_eval = {}\n",
        "img_list_eval = []\n",
        "for l in eval_list_dict:\n",
        "  label_dict_eval[l] = []\n",
        "  for obj in eval_list_dict[l]['objects']:\n",
        "    density_list_eval.append(obj)\n",
        "    label_list_eval.append(eval_list_dict[l]['objects'][obj]['category'])\n",
        "    img_list_eval.append(l)\n",
        "    label_dict_eval[l].append(eval_list_dict[l]['objects'][obj]['category'])\n",
        "  for obj in range(num_obj_dict[l],181):\n",
        "    density_list_eval.append(str(obj))\n",
        "    label_list_eval.append(str(-1))\n",
        "    img_list_eval.append(l)\n",
        "    label_dict_eval[l].append(str(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrB7k99MOqGI"
      },
      "source": [
        "print(len(bert_list_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlGkeE5VAxKg"
      },
      "source": [
        "from pandas import DataFrame\n",
        "df_train = DataFrame(density_list_train,columns=['density'])\n",
        "df_train['label'] = label_list_train\n",
        "df_train['image'] = img_list_train\n",
        "df_eval = DataFrame(density_list_eval,columns=['density'])\n",
        "df_eval['label'] = label_list_eval\n",
        "df_eval['image'] = img_list_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZsL5tzQQ3CH"
      },
      "source": [
        "label_list_train = df_train['label'].fillna('other').tolist()\n",
        "label_list_eval = df_eval['label'].fillna('other').tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dReQWB4Q7qb"
      },
      "source": [
        "print(len(label_list_train)/181)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ZNavS5Rgkn"
      },
      "source": [
        "df_train_clean = df_train[df_train['label'].notnull()][['density','label','image']]\n",
        "df_eval_clean = df_eval[df_eval['label'].notnull()][['density','label','image']] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx0NRPZSQO9"
      },
      "source": [
        "df_train_clean.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh6B7x9NP_8_"
      },
      "source": [
        "print(list_dict_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tiX_wyfIED_"
      },
      "source": [
        "label_list_train = df_train_clean['label'].to_list()\n",
        "label_list_eval = df_eval_clean['label'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXUqw4FxIn4s"
      },
      "source": [
        "id_list_train = df_train_clean['density'].to_list()\n",
        "id_list_eval = df_eval_clean['density'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu-UcP7cGcpS"
      },
      "source": [
        "img_list_train = df_train_clean['image'].to_list()\n",
        "img_list_eval = df_eval_clean['image'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CKOPtN6Iefr"
      },
      "source": [
        "obj_list_train = []\n",
        "for i in range(len(id_list_train)):\n",
        "  tem = []\n",
        "  tem = [id_list_train[i],img_list_train[i]]\n",
        "  obj_list_train.append(tem)\n",
        "obj_list_eval = []\n",
        "for i in range(len(id_list_eval)):\n",
        "  tem = []\n",
        "  tem = [id_list_eval[i],img_list_eval[i]]\n",
        "  obj_list_eval.append(tem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuplDSfsImMU"
      },
      "source": [
        "print(len(obj_list_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fal52emMue4w"
      },
      "source": [
        "## Convert label into one-hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg2jOKrAi8ho"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "num_class = len(list(set(label_list_train)))\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(np.unique(list(set(label_list_train))))\n",
        "labels_one_hot_train = {}\n",
        "for f in label_dict_train:\n",
        "  num_labels = lEnc.transform(label_dict_train[f])\n",
        "  labels_one_hot_train[f] = []\n",
        "  for l in num_labels:\n",
        "    to_add = [0]*num_class\n",
        "    to_add[l]=1\n",
        "    labels_one_hot_train[f].append(to_add)\n",
        "\n",
        "\n",
        "labels_one_hot_eval = {}\n",
        "for f in label_dict_eval:\n",
        "  num_labels = lEnc.transform(label_dict_eval[f])\n",
        "  labels_one_hot_eval[f] = []\n",
        "  for l in num_labels:\n",
        "    to_add = [0]*num_class\n",
        "    to_add[l]=1\n",
        "    labels_one_hot_eval[f].append(to_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cx9gN-_B5eI"
      },
      "source": [
        "print(len(labels_one_hot_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9sogUkt5S2Z"
      },
      "source": [
        "## Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkIDf5NqSWTB"
      },
      "source": [
        "print(new_train_list_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wapjtC3urEY"
      },
      "source": [
        "node_lists_train = {}\n",
        "for l in new_train_list_dict:\n",
        "  node_lists_train[l] = []\n",
        "  for obj in new_train_list_dict[l]['objects']:\n",
        "    node_lists_train[l].append(obj)\n",
        "node_lists_eval = {}\n",
        "for l in new_eval_list_dict:\n",
        "  node_lists_eval[l] = []\n",
        "  for obj in new_eval_list_dict[l]['objects']:\n",
        "    node_lists_eval[l].append(obj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y61IV7mvzfJI"
      },
      "source": [
        "print(node_lists_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea7iGEDQUlOh"
      },
      "source": [
        "# build local graph\n",
        "object_graph_dict_train = {}\n",
        "for l in new_train_list_dict:\n",
        "  object_graph_dict_train[l] = {}\n",
        "  for obj in new_train_list_dict[l]['objects']:\n",
        "    object_graph_dict_train[l][obj] = {}\n",
        "    object_graph_dict_train[l][obj]['obj2'] = []\n",
        "\n",
        "object_graph_dict_eval = {}\n",
        "for l in new_eval_list_dict:\n",
        "  object_graph_dict_eval[l] = {}\n",
        "  for obj in new_eval_list_dict[l]['objects']:\n",
        "    object_graph_dict_eval[l][obj] = {}\n",
        "    object_graph_dict_eval[l][obj]['obj2'] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9LkwB0nHdX-"
      },
      "source": [
        "print(len(list_dict_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jsI_KrbJBqp"
      },
      "source": [
        "print(list_dict_test.keys())\n",
        "print(list_dict_test['83594639'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLHpMbpbu3am"
      },
      "source": [
        "#fill the empty graph\n",
        "for sg in list_dict_train:\n",
        "    obj_list = list_dict_train[sg]['objects']\n",
        "    for rel_item in list_dict_train[sg]['relationships']:\n",
        "        obj1 = str(rel_item[0])\n",
        "        obj2 = str(rel_item[1])\n",
        "        object_graph_dict_train[sg][rel_item[0]]['obj2'].append(obj2)\n",
        "\n",
        "#fill the empty graph\n",
        "for sg in list_dict_test:\n",
        "    obj_list = list_dict_test[sg]['objects']\n",
        "    for rel_item in list_dict_test[sg]['relationships']:\n",
        "        obj1 = str(rel_item[0])\n",
        "        obj2 = str(rel_item[1])\n",
        "        object_graph_dict_eval[sg][rel_item[0]]['obj2'].append(obj2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1RcUHYSxpsQ"
      },
      "source": [
        "node_size_train = []\n",
        "for f in object_graph_dict_train:\n",
        "  l = object_graph_dict_train[f].keys()\n",
        "  node_size_train.append(len(l))\n",
        "\n",
        "node_size_eval = []\n",
        "for f in object_graph_dict_eval:\n",
        "  l = object_graph_dict_eval[f].keys()\n",
        "  node_size_eval.append(len(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oo9mLOsZzEU"
      },
      "source": [
        "\n",
        "#padding methods\n",
        "#weight for padding is -1\n",
        "#node id if for padding nodes is -1\n",
        "max_len = 177\n",
        "col_train = {}\n",
        "row_train = {}\n",
        "weight_train = {}\n",
        "for f in object_graph_dict_train:\n",
        "  col_train[f] = []\n",
        "  row_train[f] = []\n",
        "  weight_train[f] = []\n",
        "  for obj in object_graph_dict_train[f]:\n",
        "    obj_rel_list = object_graph_dict_train[f][obj]['obj2']\n",
        "    for obj2 in object_graph_dict_train[f]:\n",
        "      if obj2 in obj_rel_list:\n",
        "        weight_train[f].append(1.0)\n",
        "      else:\n",
        "        weight_train[f].append(0.0)\n",
        "      row_train[f].append(obj)\n",
        "      col_train[f].append(obj2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWxnskPVLJ3_"
      },
      "source": [
        "\n",
        "#padding methods\n",
        "#weight for padding is -1\n",
        "#node id if for padding nodes is -1\n",
        "max_len = 181\n",
        "col_eval = {}\n",
        "row_eval = {}\n",
        "weight_eval = {}\n",
        "for f in object_graph_dict_eval:\n",
        "  col_eval[f] = []\n",
        "  row_eval[f] = []\n",
        "  weight_eval[f] = []\n",
        "  for obj in object_graph_dict_eval[f]:\n",
        "    obj_rel_list = object_graph_dict_eval[f][obj]['obj2']\n",
        "    for obj2 in object_graph_dict_eval[f]:\n",
        "      if obj2 in obj_rel_list:\n",
        "        weight_eval[f].append(1.0)\n",
        "      else:\n",
        "        weight_eval[f].append(0.0)\n",
        "      row_eval[f].append(obj)\n",
        "      col_eval[f].append(obj2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD4wDEbP4Jmg"
      },
      "source": [
        "#PROBLEMS\n",
        "# how to determine the weight to the node itself\n",
        "# how to padding the node"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYsUdteVLz_e"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "node_size = 181\n",
        "adj_train = {}\n",
        "for f in weight_train:\n",
        "  adj_train[f] = sp.csr_matrix((weight_train[f], (row_train[f], col_train[f])), shape=(node_size, node_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Num0kxITOwK6"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "node_size = 181\n",
        "adj_eval = {}\n",
        "for f in weight_eval:\n",
        "  adj_eval[f] = sp.csr_matrix((weight_eval[f], (row_eval[f], col_eval[f])), shape=(node_size, node_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM89cDQb0j5Q"
      },
      "source": [
        "##Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBRi-q_R0npl"
      },
      "source": [
        "# from inits import *\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.support = placeholders['support']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            for i in range(len(self.support)):\n",
        "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
        "                                                        name='weights_' + str(i))\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        supports = list()\n",
        "        for i in range(len(self.support)):\n",
        "            if not self.featureless:\n",
        "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
        "                              sparse=self.sparse_inputs)\n",
        "            else:\n",
        "                pre_sup = self.vars['weights_' + str(i)]            \n",
        "            support = dot(self.support[i], pre_sup, sparse=True)\n",
        "            supports.append(support)\n",
        "        output = tf.add_n(supports)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\t\t\t\n",
        "\t\t\n",
        "        self.embedding = output #output\n",
        "        return self.act(output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkVgGB0k0xOH"
      },
      "source": [
        "# from metrics import *\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
        "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "        self.placeholders = {}\n",
        "\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "\n",
        "        self.loss = 0\n",
        "        self.accuracy = 0\n",
        "        self.optimizer = None\n",
        "        self.opt_op = None\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "\n",
        "        # Build sequential layer model\n",
        "        self.activations.append(self.inputs)\n",
        "        for layer in self.layers:\n",
        "            hidden = layer(self.activations[-1])\n",
        "            self.activations.append(hidden)\n",
        "        self.outputs = self.activations[-1]\n",
        "\n",
        "        # Store model variables for easy access\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "        # Build metrics\n",
        "        self._loss()\n",
        "        self._accuracy()\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.loss)\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "    def _loss(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _accuracy(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
        "        print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    def load(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = \"tmp/%s.ckpt\" % self.name\n",
        "        saver.restore(sess, save_path)\n",
        "        print(\"Model restored from file: %s\" % save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dztZuyxcgRK"
      },
      "source": [
        "class GCN(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(GCN, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "        self.pred = tf.argmax(self.outputs, 1)\n",
        "        self.labels = tf.argmax(self.placeholders['labels'], 1)\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                            dropout=True,\n",
        "                                            featureless=False,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "        \n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x, #\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQXtEYgr054Z"
      },
      "source": [
        "def masked_softmax_cross_entropy(preds, labels, mask):\n",
        "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
        "    print(preds)\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):\n",
        "    \"\"\"Accuracy with masking.\"\"\"\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT7yBttEfj0r"
      },
      "source": [
        "## Model Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKQpSeQ073v"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# # Set random seed\n",
        "# seed = random.randint(1, 200)\n",
        "# np.random.seed(seed)\n",
        "# tf.set_random_seed(seed)\n",
        "\n",
        "\n",
        "# Settings\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "      delattr(flags.FLAGS,name)\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_string('dataset', 'sencegraph', 'Dataset string.')\n",
        "# 'gcn', 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
        "flags.DEFINE_float('learning_rate', 0.00001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 100, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 1024, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 0,\n",
        "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
        "flags.DEFINE_integer('early_stopping', 20,\n",
        "                     'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxWFGY7LYPVM"
      },
      "source": [
        "print(node_lists_train['0000971160'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymSjZk5aTZB"
      },
      "source": [
        "id_bert_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g7zxZ7kwarT"
      },
      "source": [
        "### Bert CLS as Node Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opLAemuYZSrU"
      },
      "source": [
        "# wrong code for global level \n",
        "# transfer from node id to features\n",
        "import scipy.sparse as sp\n",
        "features_train = {}\n",
        "for l in node_lists_train:\n",
        "  features_train[l] = []\n",
        "  for node in node_lists_train[l]:\n",
        "    feature = np.array([0.0]*768)\n",
        "    feature += id_bert_dict[l][str(node)]\n",
        "    features_train[l].append(feature)\n",
        "  for i in range(len(node_lists_train[l]),181):\n",
        "    feature = np.array([0.0]*768)\n",
        "    features_train[l].append(feature)\n",
        "  features_train[l]=sp.csr_matrix(features_train[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBVUsDeWMX-P"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "features_eval = {}\n",
        "for l in node_lists_eval:\n",
        "  features_eval[l] = []\n",
        "  for node in node_lists_eval[l]:\n",
        "    feature = np.array([0.0]*768)\n",
        "    feature += id_bert_dict[l][str(node)]\n",
        "    features_eval[l].append(feature)\n",
        "  for i in range(len(node_lists_eval[l]),181):\n",
        "    feature = np.array([0.0]*768)\n",
        "    features_eval[l].append(feature)\n",
        "  features_eval[l]=sp.csr_matrix(features_eval[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp8cD8Abwgdc"
      },
      "source": [
        "### Other features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RVAH0ZUzN44"
      },
      "source": [
        "# wrong code for global level \n",
        "# transfer from node id to features\n",
        "import scipy.sparse as sp\n",
        "features_train = {}\n",
        "for l in node_lists_train:\n",
        "  features_train[l] = []\n",
        "  for node in node_lists_train[l]:\n",
        "    feature = np.array([0.0]*1024)\n",
        "    feature += np.array((id_density_dict[l][str(node)]))\n",
        "    features_train[l].append(feature)\n",
        "  for i in range(len(node_lists_train[l]),181):\n",
        "    feature = np.array([0.0]*1024)\n",
        "    features_train[l].append(feature)\n",
        "  features_train[l]=sp.csr_matrix(features_train[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njr92EuNzXJ0"
      },
      "source": [
        "# wrong code for global level \n",
        "# transfer from node id to features\n",
        "import scipy.sparse as sp\n",
        "features_eval = {}\n",
        "for l in node_lists_eval:\n",
        "  features_eval[l] = []\n",
        "  for node in node_lists_eval[l]:\n",
        "    feature = np.array([0.0]*1024)\n",
        "    feature += id_density_dict[l][str(node)]\n",
        "    features_eval[l].append(feature)\n",
        "  for i in range(len(node_lists_eval[l]),181):\n",
        "    feature = np.array([0.0]*1024)\n",
        "    features_eval[l].append(feature)\n",
        "  features_eval[l]=sp.csr_matrix(features_eval[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOR5-95JNHzx"
      },
      "source": [
        "'''\n",
        "import scipy.sparse as sp\n",
        "features = []\n",
        "for tokens in tokenize_nodes:\n",
        "  feature = np.array([0.0]*2048)\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      feature += id_density_dict[int(tokens[0])]\n",
        "    except:\n",
        "      pass\n",
        "  features.append(feature)\n",
        "features= sp.csr_matrix(features)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXT_fRkf6unl"
      },
      "source": [
        "print(len(features_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeCk4mzI3VtV"
      },
      "source": [
        "### Sparse to tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oDB45X71krq"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "for l in features_train:\n",
        "  features_train[l] = preprocess_features(features_train[l])\n",
        "  \n",
        "for l in features_eval:\n",
        "  features_eval[l] = preprocess_features(features_eval[l])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHh8Lmvof7cn"
      },
      "source": [
        "print(adj_train['91391310'][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFjyvVK3ChHf"
      },
      "source": [
        "## input setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-MQb4Sxkx6"
      },
      "source": [
        "#define input dictionary\n",
        "# num_obj_dict was defined to record the number of object in each document page\n",
        "#\n",
        "y_train = {}\n",
        "train_mask = {}\n",
        "for f in new_train_list_dict:\n",
        "  y_train[f] = np.array([[0]*num_class]*adj_train[f].shape[0])\n",
        "  y_train[f] = np.array(labels_one_hot_train[f])\n",
        "  train_mask[f] = [False]*adj_train[f].shape[0]\n",
        "  train_mask[f][:num_obj_dict[f]] = [True]*num_obj_dict[f]\n",
        "y_val = {}\n",
        "val_mask = {}\n",
        "for f in new_eval_list_dict:\n",
        "  y_val[f] = np.array([[0]*num_class]*adj_eval[f].shape[0])\n",
        "  y_val[f] = np.array(labels_one_hot_eval[f])\n",
        "  val_mask[f] = [False]*adj_eval[f].shape[0]\n",
        "  val_mask[f][:num_obj_dict[f]] = [True]*num_obj_dict[f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nn66dfE3BDH"
      },
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "support_train = {}\n",
        "for f in adj_train:\n",
        "  support_train[f] = [preprocess_adj(adj_train[f])]\n",
        "support_eval = {}\n",
        "for f in adj_eval:\n",
        "  support_eval[f] = [preprocess_adj(adj_eval[f])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEfj1gmtX7gW"
      },
      "source": [
        "print(features_train['91391310'][2][1])\n",
        "print(y_train['91391310'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpkmP403EB2"
      },
      "source": [
        "# just change the number of support gpu\n",
        "num_supports = 1\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape = (5,512)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, 5)),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0.5, shape=()),\n",
        "    # helper variable for sparse dropout\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
        "}\n",
        "\n",
        "# Create model\n",
        "#print(features[2][1])\n",
        "# placeholders: number of GCN\n",
        "\n",
        "model = GCN(placeholders, input_dim=1024, logging=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6JjEOvYKfIB"
      },
      "source": [
        "# Initialize session\n",
        "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "sess = tf.Session(config=session_conf)\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    feed_dict_val = construct_feed_dict(\n",
        "        features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels, model.layers[0].embedding, model.layers[1].embedding], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], outs_val[4], outs_val[5]\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qix0WbCOKu-D"
      },
      "source": [
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i]\n",
        "                      for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8w57b494ypu"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BHADqVKKxYf"
      },
      "source": [
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "    \n",
        "    # Construct feed dictionary\n",
        "    if epoch == FLAGS.epochs-1:\n",
        "        output_dic_train = {}\n",
        "        output_dic_test = {}\n",
        "    loss = 0\n",
        "    for f in features_train:\n",
        "        feed_dict = construct_feed_dict(\n",
        "            features_train[f], support_train[f], y_train[f], train_mask[f], placeholders)\n",
        "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "        outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
        "                         model.layers[0].embedding, model.layers[1].embedding], feed_dict=feed_dict)\n",
        "        loss+=outs[1]\n",
        "    print(loss)\n",
        "print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0k3sF4U1Iuq"
      },
      "source": [
        "for f in features_train:\n",
        "    cost, acc, pred, labels, emb1, emb2 = evaluate(\n",
        "        features_train[f], support_train[f], y_train[f], train_mask[f], placeholders)\n",
        "    output_dic_train[f] = emb1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li_eXabrc-RH"
      },
      "source": [
        "for f in features_eval:\n",
        "    cost, acc, pred, labels, emb1, emb2 = evaluate(\n",
        "        features_eval[f], support_eval[f], y_val[f], val_mask[f], placeholders)\n",
        "    output_dic_test[f] = emb1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l7qd9feTMip"
      },
      "source": [
        "output_dic_train_bert = output_dic_train\n",
        "output_dic_test_bert = output_dic_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0StgUxuUcDL"
      },
      "source": [
        "for img in output_dic_train_bert:\n",
        "  for i in range(len(train_list_dict[img]['objects'].keys())):\n",
        "    train_list_dict[img]['objects'][str(i)]['gcn_bert_large'] = output_dic_train_bert[img][i] \n",
        "\n",
        "for img in output_dic_test_bert:\n",
        "  for i in range(len(eval_list_dict[img]['objects'].keys())):\n",
        "    eval_list_dict[img]['objects'][str(i)]['gcn_bert_large'] = output_dic_test_bert[img][i] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aqUfjYjH9d"
      },
      "source": [
        "bert_list_train = []\n",
        "bert_list_test = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8p1tSiwIYR"
      },
      "source": [
        "for img in train_list_dict:\n",
        "  for obj in train_list_dict[img]['objects']:\n",
        "    c_obj = train_list_dict[img]['objects'][obj]\n",
        "    bert_list_train.append(c_obj['gcn_bert_large'])\n",
        "\n",
        "for img in eval_list_dict:\n",
        "  for obj in eval_list_dict[img]['objects']:\n",
        "    c_obj = eval_list_dict[img]['objects'][obj]\n",
        "    bert_list_test.append(c_obj['gcn_bert_large'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DdW6hofct_l"
      },
      "source": [
        "print(len(bert_list_train[0]))\n",
        "print(len(bert_list_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6temHORk6x0"
      },
      "source": [
        "df_train = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_bert_base_gcn_bert_base_train_parsing1_parsing2_pos.pkl')\n",
        "df_test = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_bert_base_gcn_bert_base_test_parsing1_parsing2_pos.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sJ9kPIYevZq"
      },
      "source": [
        "df_train['gcn_bert_large'] = bert_list_train\n",
        "df_test['gcn_bert_large'] = bert_list_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X5tbHI8g2Wu"
      },
      "source": [
        "df_train.to_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_bert_base_gcn_bert_base_train_parsing1_parsing2_pos_large.pkl')\n",
        "df_test.to_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_bert_base_gcn_bert_base_test_parsing1_parsing2_pos_large.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAV5Wpirguid"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}