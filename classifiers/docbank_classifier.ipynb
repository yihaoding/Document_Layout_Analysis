{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "docbank_BERT_base_sentence_max(bert_gcnbert)_max(v_gcnv)_max(cncd).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "eQpJTV7RSVZW"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OhO4xlwqExT"
      },
      "source": [
        "# Docbank BERT Classifier\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQpJTV7RSVZW"
      },
      "source": [
        "## Environment Setup\n",
        "Import key libraries and working envorinments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-GlywkSFegL"
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "979OUro5Eac3"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers.configuration_bert import BertConfig\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzmUaMTgi3v4"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "  global drive\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "  authenticate()\n",
        "  \n",
        "  for fileId in fileIds:    \n",
        "    \n",
        "    downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "    downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJjfftEpSkfm"
      },
      "source": [
        "## Loading the training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb1Q5N6LGK7z"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwEEXTGrKAUJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hN30ktKBmQN"
      },
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"df_train.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"df_train.pkl\", \"1jTIsRxXFXXyIuJmXQk3kRggQR0-rx2j_\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"train_visual.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"train_visual.pkl\", \"170OxyJtpy56-ByZ5wEqH0FjyxLcgi5j5\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"train_parsing1.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"train_parsing1.pkl\", \"1Nvh-_EgFx90OM_fr_e9IviZNPXB1MGXl\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omT5Rk7dCBkO"
      },
      "source": [
        "try:\n",
        "  _ = open(\"df_test.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"df_test.pkl\", \"1UOJJv22TLfDkfn1FQGODKwTI_INoQZi0\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"visual_test.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"visual_test.pkl\", \"1-7A5XxmwRFj04KhaKpdwSdLhwN8iGAyC\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awaoK0WyCeYM"
      },
      "source": [
        "train = pd.read_pickle('df_train.pkl')\n",
        "train_visual = pd.read_pickle('train_visual.pkl')\n",
        "train_parsing1 = pd.read_pickle('train_parsing1.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAybRpZKCr7t"
      },
      "source": [
        "test_dataset = pd.read_pickle('df_test.pkl')\n",
        "test_visual = pd.read_pickle('visual_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKkY8db9x97"
      },
      "source": [
        "train['parsing1'] = train_parsing1['parsing1']\n",
        "train['visual'] = train_visual['visual']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdUS1co-cca4"
      },
      "source": [
        "test_dataset['visual'] = test_visual['visual']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTsOsl4MEadB"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMlm8Eek1Ssp"
      },
      "source": [
        "test_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGcvxwWXIbfq"
      },
      "source": [
        "train['label'].replace({'abstract':0,'author':1,'caption':2,'date':3,\n",
        "                           'equation':4,'figure':5,'footer':6,'list':7,\n",
        "                           'paragraph':8,'reference':9,'section':10,'table':11,'title':12},inplace=True)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXJ6trv0m_s"
      },
      "source": [
        "test_dataset['label'].replace({'abstract':0,'author':1,'caption':2,'date':3,\n",
        "                           'equation':4,'figure':5,'footer':6,'list':7,\n",
        "                           'paragraph':8,'reference':9,'section':10,'table':11,'title':12},inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0vEmt4NFtfd"
      },
      "source": [
        "test_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baSmeDdIEadM"
      },
      "source": [
        "new_df = train[['text', 'label', 'gcn_char_density','gcn_visual_feature','gcn_char_number','gcn_bert_base','parsing1','parsing2','visual']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX1g2eqC0s4I"
      },
      "source": [
        "new_df_test = test_dataset[['text', 'label', 'gcn_char_density','gcn_visual_feature','gcn_char_number','gcn_bert_base','parsing1','parsing2','visual']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etS50Z8tlGYy"
      },
      "source": [
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUjXvioXlK3M"
      },
      "source": [
        "new_df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiQjAgdSqhn"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvXxpfNCGER2"
      },
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE*2\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "# Change the pre-trained bert model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", truncation=True, do_lower_case=True) #Cased "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vWRDemOGxJD"
      },
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.label\n",
        "        self.max_len = max_len\n",
        "        self.char_density = dataframe.gcn_char_density\n",
        "        self.char_number = dataframe.gcn_char_number\n",
        "        self.gcn_bert_cls = dataframe.gcn_bert_base\n",
        "        self.visual_feature = dataframe.gcn_visual_feature\n",
        "        self.parsing1 = dataframe.parsing1\n",
        "        self.parsing2 = dataframe.parsing2\n",
        "        self.visual = dataframe.visual\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
        "            'char_density': torch.tensor(self.char_density[index], dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index], dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index], dtype=torch.float),\n",
        "            'bert_cls': torch.tensor(self.gcn_bert_cls[index], dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index], dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index], dtype=torch.float),\n",
        "            'visual': torch.tensor(self.visual[index], dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gpe9D1QHoCd"
      },
      "source": [
        "train_size = 1\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "#test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "#print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "#testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
        "test_set = SentimentData(new_df_test,tokenizer,MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1tInLk2Eadt"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "#testing_loader = DataLoader(testing_set, **test_params)\n",
        "vali_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF12YgfxSwEr"
      },
      "source": [
        "## Define the proposed classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMqQTafXEaei"
      },
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        #bert-base-cased 768\n",
        "        #bert-large-cased bert-large-uncased 1024\n",
        "        #roberta-base-cased 768\n",
        "        #biobert\n",
        "\n",
        "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")# BERT large\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.hidden = torch.nn.Linear(768,768)\n",
        "        self.hidden0 = torch.nn.Linear(1536,1536)\n",
        "        self.hidden1 = torch.nn.Linear(1536,256)\n",
        "        self.hidden2 = torch.nn.Linear(2048,768)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(num_features=128)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(num_features=64)\n",
        "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
        "        self.pooling2 = torch.nn.MaxPool2d((4,1), stride=None)\n",
        "        self.classifier = torch.nn.Linear(256, 13)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "\n",
        "        # BERT 768 BERT / large 1024\n",
        "        \n",
        "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
        "        \n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        '''\n",
        "        density = torch.cat((char_density.unsqueeze(1),char_number.unsqueeze(1)),1)\n",
        "        density = self.pooling(density).squeeze(1)\n",
        "        density = self.hidden(density)\n",
        "        density = torch.nn.Tanh()(density)\n",
        "        density = self.dropout(density)\n",
        "\n",
        "        parsing = torch.cat((parsing1.unsqueeze(1),parsing2.unsqueeze(1)),1)\n",
        "        parsing = self.pooling(parsing).squeeze(1)\n",
        "        parsing = self.hidden(parsing)\n",
        "        parsing = torch.nn.Tanh()(parsing)\n",
        "        parsing = self.dropout(parsing)\n",
        "        '''\n",
        "        pooler = torch.cat((pooler.unsqueeze(1),bert_cls.unsqueeze(1)),1)\n",
        "        pooler = self.pooling(pooler).squeeze(1)\n",
        "        pooler = self.hidden(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        \n",
        "        visual = self.hidden2(visual)\n",
        "        visual = torch.nn.Tanh()(visual)\n",
        "        visual = self.dropout(visual)\n",
        "        \n",
        "        visual = torch.cat((visual_feature.unsqueeze(1),visual.unsqueeze(1)),1)\n",
        "        visual = self.pooling(visual).squeeze(1)\n",
        "        visual = self.hidden(visual)\n",
        "        visual = torch.nn.Tanh()(visual)\n",
        "        visual = self.dropout(visual)\n",
        "        \n",
        "        pooler = torch.cat((pooler,visual),1)\n",
        "        pooler = self.hidden0(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        pooler = self.hidden1(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        \n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pFC4c9V-8Wb"
      },
      "source": [
        "768*4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ55mIPZIkp_"
      },
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBJjGKdS2b8"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZ7YuJ5InOS"
      },
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-05) # change learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPhA2V3iIpzN"
      },
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqvtY2SIup7"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "        char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "        visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "        bert_cls = data['bert_cls'].to(device, dtype = torch.float)\n",
        "        parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "        parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "        visual = data['visual'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids, char_density,\\\n",
        "                        char_number, visual_feature, bert_cls, parsing1, parsing2, visual)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afn7xaunJHnI"
      },
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "  train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaxhthnO_ePL"
      },
      "source": [
        "output_model_file = '/content/drive/MyDrive/Docbank/bert+visual.bin'\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8lSc-jf7bU"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qD3e22vE7My"
      },
      "source": [
        "class SentimentData_test(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.max_len = max_len\n",
        "        self.char_density = dataframe.gcn_char_density\n",
        "        self.char_number = dataframe.gcn_char_number\n",
        "        self.gcn_bert_cls = dataframe.gcn_bert_base\n",
        "        self.visual_feature = dataframe.gcn_visual_feature\n",
        "        self.parsing1 = dataframe.parsing1\n",
        "        self.parsing2 = dataframe.parsing2\n",
        "        self.visual = dataframe.visual\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'char_density': torch.tensor(self.char_density[index], dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index], dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index], dtype=torch.float),\n",
        "            'bert_cls': torch.tensor(self.gcn_bert_cls[index], dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index], dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index], dtype=torch.float),\n",
        "            'visual': torch.tensor(self.visual[index], dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaw4D5UKFe-Z"
      },
      "source": [
        "new_df_true_test = test_dataset[['text', 'label', 'gcn_char_density','gcn_visual_feature','gcn_char_number','gcn_bert_base','parsing1','parsing2','visual']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0kUqjR1Fhb6"
      },
      "source": [
        "test = SentimentData_test(new_df_true_test,tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(test, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetKrn_SY-OT"
      },
      "source": [
        "def test_label_generator(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    output_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "            bert_cls = data['bert_cls'].to(device, dtype = torch.float)\n",
        "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "            visual = data['visual'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids, char_density, char_number, visual_feature, bert_cls, parsing1,parsing2,visual).squeeze()\n",
        "            \n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            output_list = output_list + list(big_idx)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            \n",
        "    return output_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ_noXfKBHmn"
      },
      "source": [
        "output = test_label_generator(model, testing_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtOYJuQh9dy0"
      },
      "source": [
        "# Get the predicted category id for selected test dataset.\n",
        "q = []\n",
        "for p in output:\n",
        "  q.append(p.cpu().numpy().tolist())\n",
        "print(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKeWt1VQ-5kH"
      },
      "source": [
        "new_df_true_test = test_dataset[['label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at5a8yvK_AGE"
      },
      "source": [
        "a = np.array(new_df_true_test).tolist()\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeNFIhOX_ets"
      },
      "source": [
        "b = []\n",
        "for p in a:\n",
        "  b.append(p[0])\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28QZsEBrbE5k"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "a = classification_report(b,q,digits=4)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
