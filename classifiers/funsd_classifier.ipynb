{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Funsd_Object_Detection_bert_large.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "eQpJTV7RSVZW",
        "cJjfftEpSkfm",
        "B7TWqvbTppX_",
        "RbiQjAgdSqhn"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQpJTV7RSVZW"
      },
      "source": [
        "## Environment Setup\n",
        "Import key libraries and working envorinments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-GlywkSFegL"
      },
      "source": [
        "!pip install transformers==4.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "979OUro5Eac3"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "#from transformers.configuration_bert import BertConfig\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9sVem7yb4MN"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "  global drive\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "  authenticate()\n",
        "  \n",
        "  for fileId in fileIds:    \n",
        "    \n",
        "    downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "    downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2-rgBQp269"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TWqvbTppX_"
      },
      "source": [
        "##Get Training and Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb5R9jPabbKD"
      },
      "source": [
        "#Do not downloading training and validation dataset at same time \n",
        "try:\n",
        "  _ = open(\"testing_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"testing_dataset.pkl\", \"1fktW64hxcjCXreMTv_2pAxSe4Nt083z3\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"training_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"training_dataset.pkl\", \"1td4mF-QxrwKF125xR5DWflGqcwn0z1LP\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_0-NGiEViK1"
      },
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"visual_train.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"visual_train.pkl\", \"1-1iI14bgFX8QbhA1uPjI7VA6kfIhPhxM\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"visual_test.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"visual_test.pkl\", \"1sEVX0nd09MfvgwrPc4KxWrClDIJpQOoz\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qiF6LKqVuje"
      },
      "source": [
        "train_visual = pd.read_pickle('visual_train.pkl')\n",
        "test_visual = pd.read_pickle('visual_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4bGVgzk-HT-"
      },
      "source": [
        "print(train_visual.shape)\n",
        "print(test_visual.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7KOu_6Dtzh"
      },
      "source": [
        "df_train = pd.read_pickle('training_dataset.pkl')\n",
        "df_test = pd.read_pickle('testing_dataset.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k5hCBUG-O1x"
      },
      "source": [
        "print(df_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzwacY0IH7ef"
      },
      "source": [
        "df_train['visual_embedding'] = test_visual['visual_embedding']\n",
        "df_test['visual_embedding'] = train_visual['visual_embedding']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNZW1muhZXBL"
      },
      "source": [
        "print(test_visual.shape)\n",
        "print(df_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DkpX4X0YxgK"
      },
      "source": [
        "test_visual.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2EV5VAOH93J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdJi7AxkpQFr"
      },
      "source": [
        "'''\n",
        "df_train.head()\n",
        "density_emb_train = df_train['char_density_emb'].tolist()\n",
        "density_emb_test = df_test['char_density_emb'].tolist()\n",
        "number_emb_train = df_train['char_number_emb'].tolist()\n",
        "number_emb_test = df_test['char_number_emb'].tolist()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GljdlwFo-IF"
      },
      "source": [
        "'''\n",
        "df_train = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_train.pkl')\n",
        "df_test = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_test.pkl')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH2iQ_QspxiY"
      },
      "source": [
        "'''\n",
        "df_train['char_density_emb'] = density_emb_train\n",
        "df_train['char_number_emb'] = number_emb_train\n",
        "df_test['char_density_emb'] = density_emb_test\n",
        "df_test['char_number_emb'] = number_emb_test\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb1Q5N6LGK7z"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baSmeDdIEadM"
      },
      "source": [
        "new_df = df_train[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
        "                   'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
        "                   'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLrlbfWr-VE_"
      },
      "source": [
        "print(new_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXgrBoaeQIaX"
      },
      "source": [
        "new_df.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX1g2eqC0s4I"
      },
      "source": [
        "new_df_test = df_test[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
        "                       'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
        "                       'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKXr_-Fh-Y5z"
      },
      "source": [
        "print(new_df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiQjAgdSqhn"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvXxpfNCGER2"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 100\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE*2\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 2e-05\n",
        "# Change the pre-trained bert model\n",
        "#tokenizer = BertTokenizer.from_pretrained('roberta-base') #Cased "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vWRDemOGxJD"
      },
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.label\n",
        "        self.visual_feature = dataframe.near_visual_feature\n",
        "        self.gcn_bert_base = dataframe.gcn_bert_base\n",
        "        self.parsing1 = dataframe.gcn_parsing1\n",
        "        self.parsing2 = dataframe.gcn_parsing2\n",
        "        self.char_density = dataframe.gcn_near_char_density\n",
        "        self.char_number = dataframe.gcn_near_char_number\n",
        "        self.pos_emb = dataframe.gcn_pos_emb\n",
        "        self.visual = dataframe.visual_embedding\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
        "            #'density': torch.tensor(self.density[index],dtype=torch.float),\n",
        "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
        "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
        "            'pos_emb': torch.tensor(self.pos_emb[index],dtype=torch.float),\n",
        "            'visual': torch.tensor(self.visual[index],dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gpe9D1QHoCd"
      },
      "source": [
        "train_size = 1\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "#test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(new_df_test.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "#testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
        "test_set = SentimentData(new_df_test,tokenizer,MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1tInLk2Eadt"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "#testing_loader = DataLoader(testing_set, **test_params)\n",
        "vali_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmPq3dnpc4ht"
      },
      "source": [
        "768*3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF12YgfxSwEr"
      },
      "source": [
        "## Define the proposed classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipKk-m3pynW6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMqQTafXEaei"
      },
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        #bert-base-cased 768\n",
        "        #bert-large-cased bert-large-uncased 1024\n",
        "        #roberta-base-cased 768\n",
        "        #biobert\n",
        "\n",
        "        self.l1 = AutoModel.from_pretrained('bert-base-uncased')# BERT large\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.h1 = torch.nn.Linear(768,768)\n",
        "        self.h2 = torch.nn.Linear(768,768)\n",
        "        self.h3 = torch.nn.Linear(768,768)\n",
        "        self.h4 = torch.nn.Linear(768,768)\n",
        "        self.hidden0 = torch.nn.Linear(2304,256)\n",
        "        self.hidden1 = torch.nn.Linear(2048,768)\n",
        "        self.hidden2 = torch.nn.Linear(3072,512)\n",
        "\n",
        "        self.hidden3 = torch.nn.Linear(3072,3072)\n",
        "        self.hidden4 = torch.nn.Linear(1024,768)\n",
        "        self.classifier = torch.nn.Linear(512, 4)\n",
        "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
        "\n",
        "    def forward(self,input_ids,attention_mask,token_type_ids,visual_feature,\\\n",
        "                char_density,char_number,parsing1,parsing2,gcn_bert_base,pos_emb,visual):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        # BERT 768 BERT / large 1024\n",
        "        \n",
        "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        other = torch.cat((pooler.unsqueeze(1),gcn_bert_base.unsqueeze(1)),1)\n",
        "        other = self.pooling(other).squeeze(1)\n",
        "        other = self.h1(other)\n",
        "        other = torch.nn.Tanh()(other)\n",
        "        other = self.dropout(other)\n",
        "        pooler = other\n",
        "\n",
        "\n",
        "        parsing = torch.cat((parsing1.unsqueeze(1),parsing2.unsqueeze(1)),1)\n",
        "        parsing = self.pooling(parsing).squeeze(1)\n",
        "        parsing = self.h2(parsing)\n",
        "        parsing = torch.nn.Tanh()(parsing)\n",
        "        parsing = self.dropout(parsing)\n",
        "\n",
        "        density = torch.cat((char_density.unsqueeze(1),char_number.unsqueeze(1)),1)\n",
        "        density = self.pooling(density).squeeze(1)\n",
        "        density = self.h3(density)\n",
        "        density = torch.nn.Tanh()(density)\n",
        "        density = self.dropout(density)\n",
        "\n",
        "        visual = self.hidden1(visual)\n",
        "        visual = torch.nn.Tanh()(visual)\n",
        "        visual = self.dropout(visual)\n",
        "\n",
        "        other = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
        "        other = self.pooling(other).squeeze(1)\n",
        "        other = self.h4(other)\n",
        "        other = torch.nn.Tanh()(other)\n",
        "        other = self.dropout(other)\n",
        "        visual = other\n",
        "\n",
        "        pooler = torch.cat((pooler,visual,parsing,char_density),1)\n",
        "        pooler = self.hidden3(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        pooler = self.hidden2(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfZj0TGctdS"
      },
      "source": [
        "768*4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ55mIPZIkp_"
      },
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBJjGKdS2b8"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZ7YuJ5InOS"
      },
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-05) # change learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPhA2V3iIpzN"
      },
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqvtY2SIup7"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    output = []\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        #char_den = data['char_density'].to(device, dtype = torch.float)\n",
        "        visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "        gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
        "        parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "        parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "        char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "        char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "        pos_emb = data['pos_emb'].to(device, dtype = torch.float)\n",
        "        visual = data['visual'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids,visual_feature,char_density,\\\n",
        "                        char_number,parsing1,parsing2,gcn_bert_base,pos_emb,visual)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHAgCduAIQRI"
      },
      "source": [
        "EPOCHS = 4\n",
        "for epoch in range(EPOCHS):\n",
        "  train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ecydbX_S6Lg"
      },
      "source": [
        "## Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFiNcy16JLwt"
      },
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    output_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "            gcn_bert_base = data['gcn_bert_large'].to(device, dtype = torch.float)\n",
        "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "            pos_emb = data['pos_emb'].to(device, dtype = torch.float)\n",
        "            visual = data['visual'].to(device, dtype = torch.float)\n",
        "            \n",
        "            outputs = model(ids, mask, token_type_ids,visual_feature,\\\n",
        "                char_density,char_number,parsing1,parsing2,gcn_bert_base,pos_emb,visual).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            output_list = output_list + list(big_idx)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu,output_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Yx1waSv6iF"
      },
      "source": [
        "acc,hidden_list = valid(model, vali_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKMELmeFUaH5"
      },
      "source": [
        "acc,hidden_list = valid(model, vali_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDxgkeaILFQ9"
      },
      "source": [
        "model_test = torch.load('funsd_od_bert_large_uncased.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8lSc-jf7bU"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qD3e22vE7My"
      },
      "source": [
        "class SentimentData_test(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.max_len = max_len\n",
        "        self.visual_feature = dataframe.near_visual_feature\n",
        "        self.gcn_bert_base = dataframe.gcn_bert_base\n",
        "        self.parsing1 = dataframe.gcn_parsing1\n",
        "        self.parsing2 = dataframe.gcn_parsing2\n",
        "        self.char_density = dataframe.gcn_near_char_density\n",
        "        self.char_number = dataframe.gcn_near_char_number\n",
        "        self.pos_emb = dataframe.gcn_pos_emb\n",
        "        self.visual = dataframe.visual_embedding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
        "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
        "            'pos_emb': torch.tensor(self.pos_emb[index],dtype=torch.float),\n",
        "            'visual': torch.tensor(self.visual[index],dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMRN5w3RXjqB"
      },
      "source": [
        "### load the test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaw4D5UKFe-Z"
      },
      "source": [
        "new_df_true_test = new_df_test[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
        "                       'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
        "                       'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0kUqjR1Fhb6"
      },
      "source": [
        "test = SentimentData_test(new_df_true_test,tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(test, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetKrn_SY-OT"
      },
      "source": [
        "def test_label_generator(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    output_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
        "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "            pos_emb = data['pos_emb'].to(device, dtype = torch.float)\n",
        "            visual = data['visual'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids,visual_feature,char_density,char_number,parsing1,parsing2,gcn_bert_base,pos_emb,visual).squeeze()\n",
        "            \n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            output_list = output_list + list(big_idx)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            \n",
        "    return output_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ_noXfKBHmn"
      },
      "source": [
        "output = test_label_generator(model, testing_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtOYJuQh9dy0"
      },
      "source": [
        "# Get the predicted category id for selected test dataset.\n",
        "q = []\n",
        "for p in output:\n",
        "  q.append(p.cpu().numpy().tolist())\n",
        "print(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnJiHq5Lm5u5"
      },
      "source": [
        "p = new_df_test['label'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrt7A_W8miu"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "report = classification_report(p,q, digits=4)\n",
        "matrix = confusion_matrix(p,q)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
